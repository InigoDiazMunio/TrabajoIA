# TrabajoIA
Este repositorio es para llevar acabo el proyecto sobre Entrenamiento distribuido de modelos de inteligencia artificial en entornos multi-GPU de la asignatura Trabajo integrado de computaciÃ³n e IA  de la Universidad de Deusto. Cualquier uso de este codigo ha de ser consultado con el creador.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa y compara diferentes estrategias de paralelizaciÃ³n para el entrenamiento de modelos de deep learning:

- **Baseline**: Entrenamiento en single GPU sin paralelizaciÃ³n
- **DataParallel**: ParalelizaciÃ³n a nivel de datos 
- **DistributedDataParallel (DDP)**: Entrenamiento distribuido multi-GPU 

## ğŸš€ InstalaciÃ³n
```bash
# Clonar repositorio
git clone <repo-url>
cd TRABAJOIA

# Instalar dependencias
pip install -r requirements.txt
```

## ğŸ’» Uso

### Ejecutar todos los experimentos:
```bash
python src/experiments/run_all_experiments.py
```

### Usar mÃ³dulos individuales:
```python
from src.datasets import get_mnist
from src.models import MLP
from src.training import train_model

# Cargar datos
train_loader, test_loader = get_mnist(batch_size=64)

# Crear modelo
model = MLP()

# Entrenar
device = 'cuda' if torch.cuda.is_available() else 'cpu'
resultados = train_model(
    model, 
    train_loader, 
    test_loader, 
    epochs=5, 
    device=device, 
    name="Mi Experimento"
)
```

## ğŸ“Š Experimentos Incluidos

1. **MNIST + MLP**: ClasificaciÃ³n bÃ¡sica
2. **MNIST + CNN Small**: CNN pequeÃ±a
3. **CIFAR-10 + CNN Medium**: CNN mediana
4. **CIFAR-100 + CNN Large**: ResNet con bloques residuales
5. **IMDB + Transformer Small**: ClasificaciÃ³n de sentimientos
6. **WikiText-103 + Transformer Medium**: Language modeling

## ğŸ‘¨â€ğŸ’» Autor

IÃ±igo Diaz Munio