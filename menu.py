"""
Script maestro para ejecutar experimentos con diferentes m√©todos de paralelizaci√≥n.
Men√∫ interactivo para elegir qu√© m√©todo ejecutar.
"""

import sys
import os
import torch

def mostrar_menu():
    """Muestra el men√∫ principal."""
    print("\n" + "="*70)
    print("  ENTRENAMIENTO DISTRIBUIDO - MEN√ö PRINCIPAL")
    print("="*70)
    print("\nM√©todos disponibles:")
    print("  1. Baseline (Single GPU)")
    print("  2. DataParallel (Multi-GPU)")
    print("  3. DistributedDataParallel / DDP (Multi-GPU Distribuido)")
    print("  4. Ejecutar TODOS los m√©todos (secuencialmente)")
    print("  5. Informaci√≥n del sistema")
    print("  0. Salir")
    print("\n" + "="*70)


def mostrar_info_sistema():
    """Muestra informaci√≥n del sistema."""
    print("\n" + "="*70)
    print("  INFORMACI√ìN DEL SISTEMA")
    print("="*70)
    print(f"\nPyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"N√∫mero de GPUs: {torch.cuda.device_count()}")
    
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            print(f"\nGPU {i}:")
            print(f"  Nombre: {torch.cuda.get_device_name(i)}")
            print(f"  Memoria: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB")
            print(f"  Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}")
    else:
        print("\n‚ö†Ô∏è  No hay GPUs disponibles. Los entrenamientos ser√°n en CPU (muy lentos).")
    
    print("\n" + "="*70)


def ejecutar_baseline():
    """Ejecuta el m√©todo baseline."""
    print("\n" + "="*70)
    print("  EJECUTANDO: Baseline (Single GPU)")
    print("="*70 + "\n")
    
    os.system("python run.py")
    
    print("\n‚úÖ Baseline completado")
    input("\nPresiona Enter para continuar...")


def ejecutar_dataparallel():
    """Ejecuta el m√©todo DataParallel."""
    num_gpus = torch.cuda.device_count()
    
    print("\n" + "="*70)
    print("  EJECUTANDO: DataParallel (Multi-GPU)")
    print("="*70)
    print(f"\nGPUs disponibles: {num_gpus}")
    
    if num_gpus < 2:
        print("\n‚ö†Ô∏è  ADVERTENCIA: Solo hay 1 GPU disponible.")
        print("   DataParallel funcionar√° pero no habr√° speedup real.")
        respuesta = input("\n¬øDeseas continuar? (s/n): ")
        if respuesta.lower() != 's':
            return
    
    print()
    os.system("python run_dataparallel.py")
    
    print("\n‚úÖ DataParallel completado")
    input("\nPresiona Enter para continuar...")


def ejecutar_ddp():
    """Ejecuta el m√©todo DDP."""
    num_gpus = torch.cuda.device_count()
    
    print("\n" + "="*70)
    print("  EJECUTANDO: DistributedDataParallel (DDP)")
    print("="*70)
    print(f"\nGPUs disponibles: {num_gpus}")
    
    if num_gpus < 2:
        print("\n‚ö†Ô∏è  ADVERTENCIA CR√çTICA: DDP requiere al menos 2 GPUs.")
        print("   Con 1 GPU, DDP no funcionar√° correctamente.")
        respuesta = input("\n¬øDeseas continuar de todos modos? (s/n): ")
        if respuesta.lower() != 's':
            return
    
    print()
    os.system("python run_ddp.py")
    
    print("\n‚úÖ DDP completado")
    input("\nPresiona Enter para continuar...")


def ejecutar_todos():
    """Ejecuta todos los m√©todos secuencialmente."""
    print("\n" + "="*70)
    print("  EJECUTANDO: TODOS LOS M√âTODOS")
    print("="*70)
    print("\nSe ejecutar√°n en orden:")
    print("  1. Baseline")
    print("  2. DataParallel")
    print("  3. DDP")
    print("\nEsto puede tomar varias horas.")
    
    respuesta = input("\n¬øEst√°s seguro de continuar? (s/n): ")
    if respuesta.lower() != 's':
        return
    
    # Ejecutar baseline
    print("\n" + "="*70)
    print("  [1/3] Ejecutando Baseline...")
    print("="*70 + "\n")
    os.system("python run.py")
    print("\n‚úÖ Baseline completado")
    
    # Ejecutar DataParallel
    print("\n" + "="*70)
    print("  [2/3] Ejecutando DataParallel...")
    print("="*70 + "\n")
    os.system("python run_dataparallel.py")
    print("\n‚úÖ DataParallel completado")
    
    # Ejecutar DDP
    print("\n" + "="*70)
    print("  [3/3] Ejecutando DDP...")
    print("="*70 + "\n")
    os.system("python run_ddp.py")
    print("\n‚úÖ DDP completado")
    
    print("\n" + "="*70)
    print("  ‚úÖ TODOS LOS M√âTODOS COMPLETADOS")
    print("="*70)
    print("\nResultados guardados en la carpeta 'results/'")
    print("Abre los archivos Excel para comparar los resultados.")
    
    input("\nPresiona Enter para continuar...")


def main():
    """Funci√≥n principal del men√∫."""
    while True:
        mostrar_menu()
        
        try:
            opcion = input("\nSelecciona una opci√≥n (0-5): ").strip()
            
            if opcion == '0':
                print("\nüëã Saliendo del programa. ¬°Hasta luego!")
                break
            
            elif opcion == '1':
                ejecutar_baseline()
            
            elif opcion == '2':
                ejecutar_dataparallel()
            
            elif opcion == '3':
                ejecutar_ddp()
            
            elif opcion == '4':
                ejecutar_todos()
            
            elif opcion == '5':
                mostrar_info_sistema()
                input("\nPresiona Enter para continuar...")
            
            else:
                print("\n‚ùå Opci√≥n inv√°lida. Por favor selecciona 0-5.")
                input("\nPresiona Enter para continuar...")
        
        except KeyboardInterrupt:
            print("\n\nüëã Programa interrumpido. ¬°Hasta luego!")
            break
        except Exception as e:
            print(f"\n‚ùå Error: {e}")
            input("\nPresiona Enter para continuar...")


if __name__ == "__main__":
    print("\n" + "="*70)
    print("  TRABAJO FIN DE GRADO - ENTRENAMIENTO DISTRIBUIDO")
    print("  I√±igo - Universidad del Pa√≠s Vasco (UPV/EHU)")
    print("="*70)
    
    main()